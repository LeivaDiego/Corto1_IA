{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examen Corto 1\n",
    "## Task Frozen Lake\n",
    "### Integrantes:\n",
    "- Diego Leiva       21752\n",
    "- Maria Ramirez     21342\n",
    "- Gustavo Gonzalez  21438"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Librerias Necesarias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.envs.toy_text.frozen_lake import generate_random_map\n",
    "import numpy as np\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Configuracion inicial**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter('ignore')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Crear el entorno Frozen Lake**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estados:\n",
    "- S: starting point, seguro\n",
    "- F: frozen surface, seguro\n",
    "- H: hole, atrapado\n",
    "- G: goal, seguro\n",
    "\n",
    "Acciones:\n",
    "- 0: Move left\n",
    "- 1: Move down\n",
    "- 2: Move right\n",
    "- 3: Move up\n",
    "\n",
    "Recompensas:\n",
    "- Reach goal: +1\n",
    "- Reach hole: 0\n",
    "- Reach frozen: 0\n",
    "\n",
    "Probabilidades:\n",
    "- P(intended) = 1/3\n",
    "- P(perpendicular direction) = 1/3\n",
    "- P(perpendicular direction) = 1/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FFFH\n",
      "FFFF\n",
      "FFFG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Crear un nuevo entorno\n",
    "env = gym.make(\"FrozenLake-v1\", \n",
    "               desc = generate_random_map(size=4),  # Generar un mapa aleatorio\n",
    "               map_name = \"4x4\",                    # Mapa 4x4\n",
    "               is_slippery = True,                  # Entorno resbaladizo\n",
    "               render_mode=\"ansi\")                  # Modo de renderizado\n",
    "env.reset()\n",
    "\n",
    "# Visualizar el entorno\n",
    "print(env.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q-Learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-table antes del entrenamiento: \n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Inicializacion de la Q table\n",
    "qtable = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "# Definicion de Hiperparametros:\n",
    "episodes = 100000\n",
    "alpha = 0.9                 # Tasa de aprendizaje\n",
    "gamma = 0.9                 # Tasa de descuento\n",
    "epsilon = 1.0               # Aletoriedad en la seleccion de acciones\n",
    "epsilon_decay = 0.0001      # Tasa de decaiminto de epsilon\n",
    "\n",
    "# Mostrar la tabla\n",
    "print('Q-table antes del entrenamiento: ')\n",
    "print(qtable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entrenamiento del Agente**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-table despues del entrenamiento:\n",
      "[[4.05470111e-02 4.07281315e-02 1.88145073e-01 4.21094627e-02]\n",
      " [4.39056696e-02 6.04556316e-02 2.08042149e-01 4.81737913e-02]\n",
      " [2.19413771e-01 3.92787758e-02 4.45510281e-02 3.88236884e-02]\n",
      " [8.15032126e-03 1.04699876e-04 8.45659030e-05 1.05106226e-01]\n",
      " [4.10274301e-02 5.77677336e-02 2.21107827e-01 4.16730961e-02]\n",
      " [4.94800620e-02 5.02155264e-02 2.61005150e-01 4.98381122e-02]\n",
      " [2.95868002e-01 6.10299049e-03 1.33866925e-02 1.23337755e-07]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [5.83938945e-02 5.82268375e-02 2.70135908e-01 4.41425478e-02]\n",
      " [6.10579710e-02 3.56676310e-01 5.89662002e-02 5.26678377e-02]\n",
      " [9.65945591e-02 4.98960964e-01 9.55185478e-02 9.64416389e-02]\n",
      " [7.53626818e-02 6.91127857e-01 9.09990986e-02 3.10826344e-02]\n",
      " [6.33726988e-02 6.78231294e-02 3.07360552e-01 6.30366640e-02]\n",
      " [8.66578066e-02 1.30465114e-01 4.17502607e-01 8.25024746e-02]\n",
      " [1.49993084e-01 1.50331711e-01 1.58447034e-01 6.10901001e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Por cada episodio, explorar el entorno\n",
    "for i in range(episodes):\n",
    "    state = env.reset()[0]\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "\n",
    "    # Mientras que el agente no caiga en un hoyo o alcanze la meta, continuar entrenando\n",
    "    while not truncated and not terminated:\n",
    "        # Generar un numero aleatorio entre 0 y 1\n",
    "        rnd = np.random.random()\n",
    "\n",
    "        # Si el aleatorio es menor a epsilon, tomar una accion aleatoria\n",
    "        if rnd < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        # de lo contrario, tomar la accion con mayor valor\n",
    "        else:\n",
    "            action = np.argmax(qtable[state])\n",
    "\n",
    "        # Ejecutar la accion y mover el agente\n",
    "        new_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        # Actualizar Q(s,a)\n",
    "        qtable[state, action] = qtable[state, action] + alpha * (\n",
    "            reward + gamma * np.max(qtable[new_state]) - qtable[state, action]\n",
    "            )\n",
    "        \n",
    "        # Actualizar el estado actual\n",
    "        state = new_state\n",
    "\n",
    "    # Actualizar el valor de epsilon\n",
    "    epsilon = max(epsilon - epsilon_decay, 0)\n",
    "\n",
    "    if epsilon == 0:\n",
    "        alpha = 0.0001\n",
    "            \n",
    "# Visualizar la Q-table actualizada\n",
    "print('Q-table despues del entrenamiento:')\n",
    "print(f\"{qtable}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluacion del entrenamiento**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasa de exito: 100.0%\n"
     ]
    }
   ],
   "source": [
    "success = 0\n",
    "\n",
    "# Evaluar la tasa de exito\n",
    "for i in range(episodes):\n",
    "    state = env.reset()[0]\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    done = truncated and terminated\n",
    "\n",
    "    while not done:\n",
    "        action = np.argmax(qtable[state])\n",
    "\n",
    "        new_state, reward, truncated, terminated, info = env.step(action)\n",
    "        done = truncated and terminated\n",
    "\n",
    "        state = new_state\n",
    "        success += reward\n",
    "\n",
    "# Obtener la tasa de exito\n",
    "print(f\"Tasa de exito: {round(success/episodes*100,2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualizar la politica aprendida**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FFFH\n",
      "FFFF\n",
      "FFFG\n",
      "\n",
      "  (Right)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "FFFF\n",
      "FFFG\n",
      "\n",
      "  (Right)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FFFH\n",
      "FFFF\n",
      "FFFG\n",
      "\n",
      "  (Right)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FFFH\n",
      "FFFF\n",
      "FFFG\n",
      "\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FFFH\n",
      "FFFF\n",
      "FFFG\n",
      "\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FFFH\n",
      "FFFF\n",
      "FFFG\n",
      "\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FFFH\n",
      "FFFF\n",
      "FFFG\n",
      "\n",
      "  (Right)\n",
      "SFFF\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "FFFF\n",
      "FFFG\n",
      "\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FFFH\n",
      "FFFF\n",
      "FFFG\n",
      "\n",
      "  (Right)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FFFH\n",
      "FFFF\n",
      "FFFG\n",
      "\n",
      "  (Left)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FFFH\n",
      "FFFF\n",
      "FFFG\n",
      "\n",
      "  (Right)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FFFH\n",
      "FFFF\n",
      "FFFG\n",
      "\n",
      "  (Left)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FFFH\n",
      "FFFF\n",
      "FFFG\n",
      "\n",
      "  (Left)\n",
      "SFFF\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "FFFF\n",
      "FFFG\n",
      "\n",
      "  (Left)\n",
      "SFFF\n",
      "FFFH\n",
      "FF\u001b[41mF\u001b[0mF\n",
      "FFFG\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "FFFH\n",
      "FFF\u001b[41mF\u001b[0m\n",
      "FFFG\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "FFFH\n",
      "FFFF\n",
      "FFF\u001b[41mG\u001b[0m\n",
      "\n",
      "Secuencia: ['RIGHT', 'RIGHT', 'RIGHT', 'RIGHT', 'RIGHT', 'RIGHT', 'RIGHT', 'RIGHT', 'RIGHT', 'LEFT', 'RIGHT', 'LEFT', 'LEFT', 'LEFT', 'DOWN', 'DOWN']\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()[0]\n",
    "terminated = False\n",
    "truncated = False\n",
    "\n",
    "sequence = []\n",
    "print(env.render())\n",
    "\n",
    "while not terminated and not truncated:\n",
    "    if np.max(qtable[state]) > 0:\n",
    "      action = np.argmax(qtable[state])\n",
    "    else:\n",
    "      action = env.action_space.sample()\n",
    "    \n",
    "    # Agregar la accion a la secuencia\n",
    "    sequence.append(action)\n",
    "\n",
    "    # Ejecutar la accion y mover el agente\n",
    "    new_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    # Actualizar el estado actual\n",
    "    state = new_state\n",
    "\n",
    "    # Visualizar el movimiento\n",
    "    print(env.render())\n",
    "\n",
    "\n",
    "# Definir el mapeo de acciones\n",
    "action_mapping = {\n",
    "    0: 'LEFT',\n",
    "    1: 'DOWN',\n",
    "    2: 'RIGHT',\n",
    "    3: 'UP'\n",
    "}\n",
    "\n",
    "# Mapear los valores numericos a direcciones\n",
    "mapped_sequence = [action_mapping[action] for action in sequence]\n",
    "\n",
    "# Mostrar la secuencia\n",
    "print(f\"Secuencia: {mapped_sequence}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
