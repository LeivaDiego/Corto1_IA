{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examen Corto 1\n",
    "## Task Frozen Lake\n",
    "### Integrantes:\n",
    "- Diego Leiva       21752\n",
    "- Maria Ramirez     21342\n",
    "- Gustavo Gonzalez  21438"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Librerias Necesarias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.envs.toy_text.frozen_lake import generate_random_map\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Crear el entorno Frozen Lake**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estados:\n",
    "- S: starting point, seguro\n",
    "- F: frozen surface, seguro\n",
    "- H: hole, atrapado\n",
    "- G: goal, seguro\n",
    "\n",
    "Acciones:\n",
    "- 0: Move left\n",
    "- 1: Move down\n",
    "- 2: Move right\n",
    "- 3: Move up\n",
    "\n",
    "Recompensas:\n",
    "- Reach goal: +1\n",
    "- Reach hole: 0\n",
    "- Reach frozen: 0\n",
    "\n",
    "Probabilidades:\n",
    "- P(intended) = 1/3\n",
    "- P(perpendicular direction) = 1/3\n",
    "- P(perpendicular direction) = 1/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "HFFF\n",
      "FFFF\n",
      "FFFG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Crear un nuevo entorno\n",
    "env = gym.make(\"FrozenLake-v1\", \n",
    "               desc = generate_random_map(size=4),  # Generar un mapa aleatorio\n",
    "               map_name = \"4x4\",                    # Mapa 4x4\n",
    "               is_slippery = True,                  # Entorno resbaladizo\n",
    "               render_mode=\"ansi\")                  # Modo de renderizado\n",
    "env.reset()\n",
    "\n",
    "# Visualizar el entorno\n",
    "print(env.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q-Learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-table antes del entrenamiento: \n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "qtable = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "# Hiperparametros:\n",
    "episodes = 100000\n",
    "alpha = 0.9                 # Tasa de aprendizaje\n",
    "gamma = 0.9                 # Tasa de descuento\n",
    "epsilon = 1.0               # Aletoriedad en la seleccion de acciones\n",
    "epsilon_decay = 0.0001      # Tasa de decaiminto de epsilon\n",
    "\n",
    "# Mostrar la tabla\n",
    "print('Q-table antes del entrenamiento: ')\n",
    "print(qtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-table despues del entrenamiento:\n",
      "[[0.0085728  0.00422405 0.02199462 0.15152207]\n",
      " [0.06427551 0.06476317 0.19685767 0.0685083 ]\n",
      " [0.09113729 0.22488153 0.09176423 0.08753788]\n",
      " [0.09896328 0.26227364 0.09698403 0.09116873]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.01536578 0.02036773 0.23067482 0.0036164 ]\n",
      " [0.09172661 0.27659024 0.08863807 0.09295948]\n",
      " [0.10320385 0.35207497 0.11028755 0.11190699]\n",
      " [0.02552849 0.26190339 0.01244113 0.01925618]\n",
      " [0.09689198 0.10268948 0.29347005 0.10171718]\n",
      " [0.11045631 0.11984497 0.12216131 0.33477529]\n",
      " [0.53899967 0.16971948 0.1586009  0.16724307]\n",
      " [0.10469197 0.10316571 0.29174127 0.10107233]\n",
      " [0.10640664 0.40436963 0.10594869 0.10524147]\n",
      " [0.16787045 0.65371116 0.17307192 0.16152586]\n",
      " [0.         0.         0.         0.        ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento\n",
    "for i in range(episodes):\n",
    "    state = env.reset()[0]\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "\n",
    "    # Mientras que el agente no caiga en un hoyo o alcanze la meta, continuar entrenando\n",
    "    while not truncated and not terminated:\n",
    "        # Generar un numero aleatorio entre 0 y 1\n",
    "        rnd = np.random.random()\n",
    "\n",
    "        # Si el aleatorio es menor a epsilon, tomar una accion aleatoria\n",
    "        if rnd < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        # de lo contrario, tomar la accion con mayor valor\n",
    "        else:\n",
    "            action = np.argmax(qtable[state])\n",
    "\n",
    "        # Ejecutar la accion y mover el agente\n",
    "        new_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        # Actualizar Q(s,a)\n",
    "        qtable[state, action] = qtable[state, action] + alpha * (\n",
    "            reward + gamma * np.max(qtable[new_state]) - qtable[state, action]\n",
    "            )\n",
    "        \n",
    "        # Actualizar el estado actual\n",
    "        state = new_state\n",
    "\n",
    "    # Actualizar el valor de epsilon\n",
    "    epsilon = max(epsilon - epsilon_decay, 0)\n",
    "\n",
    "    if epsilon == 0:\n",
    "        alpha = 0.0001\n",
    "            \n",
    "# Visualizar la Q-table actualizada\n",
    "print('Q-table despues del entrenamiento:')\n",
    "print(f\"{qtable}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encontrar la secuencia**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasa de exito: 100.0%\n"
     ]
    }
   ],
   "source": [
    "success = 0\n",
    "# Evaluar la tasa de exito\n",
    "for i in range(episodes):\n",
    "    state = env.reset()[0]\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    done = truncated and terminated\n",
    "\n",
    "    while not done:\n",
    "        action = np.argmax(qtable[state])\n",
    "\n",
    "        new_state, reward, truncated, terminated, info = env.step(action)\n",
    "        done = truncated and terminated\n",
    "\n",
    "        state = new_state\n",
    "        success += reward\n",
    "\n",
    "# Obtener la tasa de exito\n",
    "print(f\"Tasa de exito: {round(success/episodes*100,2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "HFFF\n",
      "FFFF\n",
      "FFFG\n",
      "\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "HFFF\n",
      "FFFF\n",
      "FFFG\n",
      "\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "HFFF\n",
      "FFFF\n",
      "FFFG\n",
      "\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "HFFF\n",
      "FFFF\n",
      "FFFG\n",
      "\n",
      "  (Right)\n",
      "SFFF\n",
      "H\u001b[41mF\u001b[0mFF\n",
      "FFFF\n",
      "FFFG\n",
      "\n",
      "  (Right)\n",
      "SFFF\n",
      "HF\u001b[41mF\u001b[0mF\n",
      "FFFF\n",
      "FFFG\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "H\u001b[41mF\u001b[0mFF\n",
      "FFFF\n",
      "FFFG\n",
      "\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "HFFF\n",
      "FFFF\n",
      "FFFG\n",
      "\n",
      "  (Right)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "HFFF\n",
      "FFFF\n",
      "FFFG\n",
      "\n",
      "  (Down)\n",
      "SFF\u001b[41mF\u001b[0m\n",
      "HFFF\n",
      "FFFF\n",
      "FFFG\n",
      "\n",
      "  (Down)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "HFFF\n",
      "FFFF\n",
      "FFFG\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "HF\u001b[41mF\u001b[0mF\n",
      "FFFF\n",
      "FFFG\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "HFFF\n",
      "FF\u001b[41mF\u001b[0mF\n",
      "FFFG\n",
      "\n",
      "  (Up)\n",
      "SFFF\n",
      "HFFF\n",
      "F\u001b[41mF\u001b[0mFF\n",
      "FFFG\n",
      "\n",
      "  (Right)\n",
      "SFFF\n",
      "HFFF\n",
      "FF\u001b[41mF\u001b[0mF\n",
      "FFFG\n",
      "\n",
      "  (Up)\n",
      "SFFF\n",
      "HFFF\n",
      "FFF\u001b[41mF\u001b[0m\n",
      "FFFG\n",
      "\n",
      "  (Left)\n",
      "SFFF\n",
      "HFFF\n",
      "FFFF\n",
      "FFF\u001b[41mG\u001b[0m\n",
      "\n",
      "Secuencia = [3, 3, 2, 2, 2, 1, 2, 2, 1, 1, 1, 1, 3, 2, 3, 0]\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()[0]\n",
    "terminated = False\n",
    "truncated = False\n",
    "\n",
    "sequence = []\n",
    "print(env.render())\n",
    "\n",
    "while not terminated and not truncated:\n",
    "    if np.max(qtable[state]) > 0:\n",
    "      action = np.argmax(qtable[state])\n",
    "    else:\n",
    "      action = env.action_space.sample()\n",
    "    \n",
    "    # Add the action to the sequence\n",
    "    sequence.append(action)\n",
    "\n",
    "    # Implement this action and move the agent in the desired direction\n",
    "    new_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    # Update our current state\n",
    "    state = new_state\n",
    "\n",
    "    print(env.render())\n",
    "\n",
    "print(f\"Secuencia = {sequence}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
