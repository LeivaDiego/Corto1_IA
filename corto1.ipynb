{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examen Corto 1\n",
    "## Task Frozen Lake\n",
    "### Integrantes:\n",
    "- Diego Leiva       21752\n",
    "- Maria Ramirez     21342\n",
    "- Gustavo Gonzalez  21438"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Librerias Necesarias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.envs.toy_text.frozen_lake import generate_random_map\n",
    "import numpy as np\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Configuracion inicial**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter('ignore')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Crear el entorno Frozen Lake**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estados:\n",
    "- S: starting point, seguro\n",
    "- F: frozen surface, seguro\n",
    "- H: hole, atrapado\n",
    "- G: goal, seguro\n",
    "\n",
    "Acciones:\n",
    "- 0: Move left\n",
    "- 1: Move down\n",
    "- 2: Move right\n",
    "- 3: Move up\n",
    "\n",
    "Recompensas:\n",
    "- Reach goal: +1\n",
    "- Reach hole: 0\n",
    "- Reach frozen: 0\n",
    "\n",
    "Probabilidades:\n",
    "- P(intended) = 1/3\n",
    "- P(perpendicular direction) = 1/3\n",
    "- P(perpendicular direction) = 1/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFH\n",
      "FFFF\n",
      "FFFF\n",
      "HFFG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Crear un nuevo entorno\n",
    "env = gym.make(\"FrozenLake-v1\", \n",
    "               desc = generate_random_map(size=4),  # Generar un mapa aleatorio\n",
    "               map_name = \"4x4\",                    # Mapa 4x4\n",
    "               is_slippery = True,                  # Entorno resbaladizo\n",
    "               render_mode=\"ansi\")                  # Modo de renderizado\n",
    "env.reset()\n",
    "\n",
    "# Visualizar el entorno\n",
    "print(env.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q-Learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-table antes del entrenamiento: \n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Inicializacion de la Q table\n",
    "qtable = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "# Definicion de Hiperparametros:\n",
    "episodes = 100000\n",
    "alpha = 0.9                 # Tasa de aprendizaje\n",
    "gamma = 0.9                 # Tasa de descuento\n",
    "epsilon = 1.0               # Aletoriedad en la seleccion de acciones\n",
    "epsilon_decay = 0.0001      # Tasa de decaiminto de epsilon\n",
    "\n",
    "# Mostrar la tabla\n",
    "print('Q-table antes del entrenamiento: ')\n",
    "print(qtable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entrenamiento del Agente**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-table despues del entrenamiento:\n",
      "[[0.04001333 0.20408121 0.04221602 0.06042139]\n",
      " [0.05118376 0.04343698 0.24549148 0.05124135]\n",
      " [0.26802126 0.00794028 0.00449194 0.02506428]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.05210756 0.05342699 0.05118824 0.222903  ]\n",
      " [0.05766661 0.06039997 0.29934615 0.06025739]\n",
      " [0.06802976 0.06632403 0.37476753 0.09758846]\n",
      " [0.056617   0.46856279 0.03917581 0.04038703]\n",
      " [0.0088867  0.01204284 0.01753667 0.06891997]\n",
      " [0.05083247 0.05720338 0.370922   0.07217753]\n",
      " [0.11241061 0.50640837 0.12603504 0.12838157]\n",
      " [0.14955211 0.69522522 0.16528515 0.10150858]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.04697917 0.05294616 0.42518185 0.06980289]\n",
      " [0.19999408 0.19755945 0.24083521 0.61205643]\n",
      " [0.         0.         0.         0.        ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Por cada episodio, explorar el entorno\n",
    "for i in range(episodes):\n",
    "    state = env.reset()[0]\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "\n",
    "    # Mientras que el agente no caiga en un hoyo o alcanze la meta, continuar entrenando\n",
    "    while not truncated and not terminated:\n",
    "        # Generar un numero aleatorio entre 0 y 1\n",
    "        rnd = np.random.random()\n",
    "\n",
    "        # Si el aleatorio es menor a epsilon, tomar una accion aleatoria\n",
    "        if rnd < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        # de lo contrario, tomar la accion con mayor valor\n",
    "        else:\n",
    "            action = np.argmax(qtable[state])\n",
    "\n",
    "        # Ejecutar la accion y mover el agente\n",
    "        new_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        # Actualizar Q(s,a)\n",
    "        qtable[state, action] = qtable[state, action] + alpha * (\n",
    "            reward + gamma * np.max(qtable[new_state]) - qtable[state, action]\n",
    "            )\n",
    "        \n",
    "        # Actualizar el estado actual\n",
    "        state = new_state\n",
    "\n",
    "    # Actualizar el valor de epsilon\n",
    "    epsilon = max(epsilon - epsilon_decay, 0)\n",
    "\n",
    "    if epsilon == 0:\n",
    "        alpha = 0.0001\n",
    "            \n",
    "# Visualizar la Q-table actualizada\n",
    "print('Q-table despues del entrenamiento:')\n",
    "print(f\"{qtable}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluacion del entrenamiento**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasa de exito: 100.0%\n"
     ]
    }
   ],
   "source": [
    "success = 0\n",
    "\n",
    "# Evaluar la tasa de exito\n",
    "for i in range(episodes):\n",
    "    state = env.reset()[0]\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    done = truncated and terminated\n",
    "\n",
    "    while not done:\n",
    "        action = np.argmax(qtable[state])\n",
    "\n",
    "        new_state, reward, truncated, terminated, info = env.step(action)\n",
    "        done = truncated and terminated\n",
    "\n",
    "        state = new_state\n",
    "        success += reward\n",
    "\n",
    "# Obtener la tasa de exito\n",
    "print(f\"Tasa de exito: {round(success/episodes*100,2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualizar la politica aprendida**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFH\n",
      "FFFF\n",
      "FFFF\n",
      "HFFG\n",
      "\n",
      "  (Down)\n",
      "S\u001b[41mF\u001b[0mFH\n",
      "FFFF\n",
      "FFFF\n",
      "HFFG\n",
      "\n",
      "  (Right)\n",
      "SFFH\n",
      "F\u001b[41mF\u001b[0mFF\n",
      "FFFF\n",
      "HFFG\n",
      "\n",
      "  (Right)\n",
      "SFFH\n",
      "FF\u001b[41mF\u001b[0mF\n",
      "FFFF\n",
      "HFFG\n",
      "\n",
      "  (Right)\n",
      "SFFH\n",
      "FFF\u001b[41mF\u001b[0m\n",
      "FFFF\n",
      "HFFG\n",
      "\n",
      "  (Down)\n",
      "SFFH\n",
      "FF\u001b[41mF\u001b[0mF\n",
      "FFFF\n",
      "HFFG\n",
      "\n",
      "  (Right)\n",
      "SF\u001b[41mF\u001b[0mH\n",
      "FFFF\n",
      "FFFF\n",
      "HFFG\n",
      "\n",
      "  (Left)\n",
      "SF\u001b[41mF\u001b[0mH\n",
      "FFFF\n",
      "FFFF\n",
      "HFFG\n",
      "\n",
      "  (Left)\n",
      "SF\u001b[41mF\u001b[0mH\n",
      "FFFF\n",
      "FFFF\n",
      "HFFG\n",
      "\n",
      "  (Left)\n",
      "S\u001b[41mF\u001b[0mFH\n",
      "FFFF\n",
      "FFFF\n",
      "HFFG\n",
      "\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFH\n",
      "FFFF\n",
      "FFFF\n",
      "HFFG\n",
      "\n",
      "  (Right)\n",
      "SFFH\n",
      "F\u001b[41mF\u001b[0mFF\n",
      "FFFF\n",
      "HFFG\n",
      "\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFH\n",
      "FFFF\n",
      "FFFF\n",
      "HFFG\n",
      "\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFH\n",
      "FFFF\n",
      "FFFF\n",
      "HFFG\n",
      "\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFH\n",
      "FFFF\n",
      "FFFF\n",
      "HFFG\n",
      "\n",
      "  (Right)\n",
      "SF\u001b[41mF\u001b[0mH\n",
      "FFFF\n",
      "FFFF\n",
      "HFFG\n",
      "\n",
      "  (Left)\n",
      "SFFH\n",
      "FF\u001b[41mF\u001b[0mF\n",
      "FFFF\n",
      "HFFG\n",
      "\n",
      "  (Right)\n",
      "SF\u001b[41mF\u001b[0mH\n",
      "FFFF\n",
      "FFFF\n",
      "HFFG\n",
      "\n",
      "  (Left)\n",
      "S\u001b[41mF\u001b[0mFH\n",
      "FFFF\n",
      "FFFF\n",
      "HFFG\n",
      "\n",
      "  (Right)\n",
      "SFFH\n",
      "F\u001b[41mF\u001b[0mFF\n",
      "FFFF\n",
      "HFFG\n",
      "\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFH\n",
      "FFFF\n",
      "FFFF\n",
      "HFFG\n",
      "\n",
      "  (Right)\n",
      "SFFH\n",
      "F\u001b[41mF\u001b[0mFF\n",
      "FFFF\n",
      "HFFG\n",
      "\n",
      "  (Right)\n",
      "SFFH\n",
      "FF\u001b[41mF\u001b[0mF\n",
      "FFFF\n",
      "HFFG\n",
      "\n",
      "  (Right)\n",
      "SF\u001b[41mF\u001b[0mH\n",
      "FFFF\n",
      "FFFF\n",
      "HFFG\n",
      "\n",
      "  (Left)\n",
      "SFFH\n",
      "FF\u001b[41mF\u001b[0mF\n",
      "FFFF\n",
      "HFFG\n",
      "\n",
      "  (Right)\n",
      "SFFH\n",
      "FFFF\n",
      "FF\u001b[41mF\u001b[0mF\n",
      "HFFG\n",
      "\n",
      "  (Down)\n",
      "SFFH\n",
      "FFFF\n",
      "FFF\u001b[41mF\u001b[0m\n",
      "HFFG\n",
      "\n",
      "  (Down)\n",
      "SFFH\n",
      "FFFF\n",
      "FFF\u001b[41mF\u001b[0m\n",
      "HFFG\n",
      "\n",
      "  (Down)\n",
      "SFFH\n",
      "FFFF\n",
      "FF\u001b[41mF\u001b[0mF\n",
      "HFFG\n",
      "\n",
      "  (Down)\n",
      "SFFH\n",
      "FFFF\n",
      "FFFF\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "\n",
      "  (Up)\n",
      "SFFH\n",
      "FFFF\n",
      "FFFF\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "\n",
      "  (Right)\n",
      "SFFH\n",
      "FFFF\n",
      "FFFF\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "\n",
      "  (Up)\n",
      "SFFH\n",
      "FFFF\n",
      "FFFF\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "\n",
      "Secuencia: ['DOWN', 'RIGHT', 'RIGHT', 'RIGHT', 'DOWN', 'RIGHT', 'LEFT', 'LEFT', 'LEFT', 'RIGHT', 'RIGHT', 'RIGHT', 'RIGHT', 'RIGHT', 'RIGHT', 'LEFT', 'RIGHT', 'LEFT', 'RIGHT', 'RIGHT', 'RIGHT', 'RIGHT', 'RIGHT', 'LEFT', 'RIGHT', 'DOWN', 'DOWN', 'DOWN', 'DOWN', 'UP', 'RIGHT', 'UP']\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()[0]\n",
    "terminated = False\n",
    "truncated = False\n",
    "\n",
    "sequence = []\n",
    "print(env.render())\n",
    "\n",
    "while not terminated and not truncated:\n",
    "    if np.max(qtable[state]) > 0:\n",
    "      action = np.argmax(qtable[state])\n",
    "    else:\n",
    "      action = env.action_space.sample()\n",
    "    \n",
    "    # Agregar la accion a la secuencia\n",
    "    sequence.append(action)\n",
    "\n",
    "    # Ejecutar la accion y mover el agente\n",
    "    new_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    # Actualizar el estado actual\n",
    "    state = new_state\n",
    "\n",
    "    # Visualizar el movimiento\n",
    "    print(env.render())\n",
    "\n",
    "\n",
    "# Definir el mapeo de acciones\n",
    "action_mapping = {\n",
    "    0: 'LEFT',\n",
    "    1: 'DOWN',\n",
    "    2: 'RIGHT',\n",
    "    3: 'UP'\n",
    "}\n",
    "\n",
    "# Mapear los valores numericos a direcciones\n",
    "mapped_sequence = [action_mapping[action] for action in sequence]\n",
    "\n",
    "# Mostrar la secuencia\n",
    "print(f\"Secuencia: {mapped_sequence}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
